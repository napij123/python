{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"cookies_are\": \"working\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 传递cookies\n",
    "import requests\n",
    "url = 'http://httpbin.org/cookies'\n",
    "rsp = requests.get(url,cookies=dict(cookies_are='working'))\n",
    "print(rsp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"cookies\": {\n",
      "    \"sessioncookie\": \"123\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 同一个session实例发出的多个请求保持cookies\n",
    "import requests\n",
    "\n",
    "ss = requests.Session()\n",
    "ss.get('http://httpbin.org/cookies/set/sessioncookie/123')\n",
    "r = ss.get('http://httpbin.org/cookies')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.18.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Authorization': 'Basic dXNlcjpzYWx0'}\n",
      "{'User-Agent': 'python-requests/2.18.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'x-test': 'true', 'Authorization': 'Basic dXNlcjpzYWx0bWFu'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "s = requests.Session()\n",
    "s.auth = ('user','salt')\n",
    "s.headers.update({'x-test': 'true'})\n",
    "r = s.get('http://httpbin.org/headers', headers={'x-test': None})\n",
    "'''\n",
    "如果设置相同的'x-text'则会覆盖上面设置的header\n",
    "如果想取消session的某个参数，可以在传递一个相同key，value为None\n",
    "'''\n",
    "print(r.request.headers)\n",
    "\n",
    "r = s.get('http://httpbin.org/headers',auth=('user','saltman'))\n",
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "******************************\n",
      "<_sre.SRE_Match object; span=(3, 5), match='12'>\n",
      "12\n",
      "3\n",
      "5\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "python中正则模块是re\n",
    "使用大致步骤：\n",
    "1. compile函数将正则表达式的字符串变为一个Pattern对象\n",
    "2. 通过Pattern对象的一些列方法对文本进行匹配，匹配结果是一个Match对象\n",
    "3. 用Match对象的方法，对结果进行操纵\n",
    "'''\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# \\d表示以数字\n",
    "# 后面+号表示这个数字可以出现一次或者多次\n",
    "s = r\"\\d+\" # r表示后面是原生字符串，后面不需要转义\n",
    "\n",
    "\n",
    "# 返回Pattern对象\n",
    "pattern = re.compile(s)\n",
    "\n",
    "# 返回一个Match对象\n",
    "# 默认找到一个匹配就返回\n",
    "m = pattern.match(\"one12two2three3\")\n",
    "\n",
    "# 默认匹配从头部开始，所以此次结果为None\n",
    "print(m)\n",
    "print('*'*30)\n",
    "\n",
    "# 后面为位置参数含义是从哪个位置开始查找，找到哪个位置结束\n",
    "m = pattern.match(\"one12two2three3\", 3, 20)\n",
    "\n",
    "print(m)\n",
    "print(m.group())  # match一次匹配\n",
    "print(m.start(0))\n",
    "print(m.end(0))\n",
    "print(m.span(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "******************************\n",
      "20\n",
      "******************************\n",
      "['20', '174']\n",
      "******************************\n",
      "<class 'callable_iterator'>\n",
      "******************************\n",
      "20\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\d+')\n",
    "# search 也是一次匹配\n",
    "s = pattern.search('i am 20 years,174 high')\n",
    "print(s.group())\n",
    "print('*'*30)\n",
    "\n",
    "s = pattern.search('i am 20 years,174 high',5,40)\n",
    "print(s.group())\n",
    "print('*'*30)\n",
    "\n",
    "s = pattern.findall('i am 20 years,174 high')\n",
    "print(s)\n",
    "print('*'*30)\n",
    "\n",
    "s = pattern.finditer('i am 20 years,174 high')\n",
    "print(type(s))\n",
    "print('*'*30)\n",
    "\n",
    "for i in s:\n",
    "    print(i.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\d+')\n",
    "s = pattern.split('one1two2three3four4')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html><body><div>\\n    <ul>\\n        <li class=\"item-0\"> <a href=\"0.html\"> first item </a></li>\\n        <li class=\"item-1\"> <a href=\"1.html\"> first item </a></li>\\n        <li class=\"item-2\"> <a href=\"2.html\"> first item </a></li>\\n        <li class=\"item-3\"> <a href=\"3.html\"> first item </a></li>\\n        <li class=\"item-4\"> <a href=\"4.html\"> first item </a></li>\\n        <li class=\"item-5\"> <a href=\"5.html\"> first item </a>\\n    </li></ul>\\n</div>\\n</body></html>'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from lxml import etree\n",
    "\n",
    "'''\n",
    "用lxml来解析HTML代码\n",
    "'''\n",
    "\n",
    "text = '''\n",
    "<div>\n",
    "    <ul>\n",
    "        <li class=\"item-0\"> <a href=\"0.html\"> first item </a></li>\n",
    "        <li class=\"item-1\"> <a href=\"1.html\"> first item </a></li>\n",
    "        <li class=\"item-2\"> <a href=\"2.html\"> first item </a></li>\n",
    "        <li class=\"item-3\"> <a href=\"3.html\"> first item </a></li>\n",
    "        <li class=\"item-4\"> <a href=\"4.html\"> first item </a></li>\n",
    "        <li class=\"item-5\"> <a href=\"5.html\"> first item </a>\n",
    "    </ul>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# 利用etree.HTML把字符串解析成HTML文档\n",
    "html = etree.HTML(text)\n",
    "s = etree.tostring(html)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lxml.etree._ElementTree'>\n",
      "[<Element bookstore at 0x80f7988>]\n",
      "******************************\n",
      "[<Element book at 0x80f7bc8>, <Element book at 0x80f7c08>, <Element book at 0x80f7c48>]\n",
      "******************************\n",
      "<class 'list'>\n",
      "Everyday Italian\n",
      "Python is Python\n",
      "Running\n",
      "******************************\n",
      "<class 'list'>\n",
      "Python is Python\n",
      "Running\n",
      "****************************************\n",
      "<class 'list'>\n",
      "<Element book at 0x80f7d48>\n",
      "******************************\n",
      "<class 'list'>\n",
      "23\n",
      "********************\n",
      "<class 'list'>\n",
      "Food War\n",
      "Salt Man\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# 只能读取xml格式内容，html报错\n",
    "html = etree.parse(\"./v30.txt\")\n",
    "print(type(html))\n",
    "\n",
    "rst = html.xpath('/bookstore')\n",
    "print(rst)\n",
    "print('*'*30)\n",
    "\n",
    "rst = html.xpath('/bookstore/book')\n",
    "print(rst)\n",
    "print('*'*30)\n",
    "\n",
    "rst = html.xpath('/bookstore/book/title')\n",
    "print(type(rst))\n",
    "for i in rst:\n",
    "    print(i.text)   \n",
    "print('*'*30)\n",
    "\n",
    "rst = html.xpath('/bookstore/book[price>40]/title')\n",
    "print(type(rst))\n",
    "for i in rst:\n",
    "    print(i.text)\n",
    "print('*'*40)\n",
    "\n",
    "# xpath的意思是，查找带有category属性值为sport的book元素\n",
    "rst = html.xpath('//book[@category=\"sport\"]')\n",
    "print(type(rst))\n",
    "for i in rst:\n",
    "    print(i)\n",
    "print('*'*30)\n",
    "\n",
    "rst = html.xpath('//book[@category=\"cooking\"]/price')\n",
    "print(type(rst))\n",
    "for i in rst:\n",
    "    print(i.text)\n",
    "print('*'*20)    \n",
    "\n",
    "# xpath的意识是，查找带有category属性值为sport的book元素下的year元素\n",
    "rst = html.xpath('//book[@category=\"education\"]/author')\n",
    "print(type(rst))\n",
    "for i in rst:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "<title>The Dormouse's story</title>\n",
      "title\n",
      "The Dormouse's story\n",
      "head\n",
      "========================\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "['title']\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "========================\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_doc = '''<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html_doc,'html.parser')\n",
    "\n",
    "#print(soup.prettify())\n",
    "\n",
    "print('=='*12)\n",
    "print(soup.title)\n",
    "print(soup.title.name)\n",
    "print(soup.title.string)\n",
    "print(soup.title.parent.name)\n",
    "print('=='*12)\n",
    "\n",
    "print(soup.p)\n",
    "print(soup.p['class'])\n",
    "print(soup.a)\n",
    "print(soup.find_all('a'))\n",
    "print(soup.find(id=\"link3\"))\n",
    "print('=='*12)\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "    \n",
    "print(soup.get_text())  # 从文档中获取所有文字内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 id=\"title\">Hello World</h1>]\n",
      "<h1 id=\"title\">Hello World</h1>\n",
      "Hello World\n",
      "========================\n",
      "[<a class=\"link\" href=\"#\">This is link1</a>, <a class=\"link\" href=\"# link2\">This is link2</a>]\n",
      "This is link1\n",
      "#\n",
      "This is link2\n",
      "# link2\n",
      "========================\n",
      "[<h1 id=\"title\">Hello World</h1>]\n",
      "[<a class=\"link\" href=\"#\">This is link1</a>, <a class=\"link\" href=\"# link2\">This is link2</a>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html_doc = '''\n",
    "<html><body><h1 id=\"title\">Hello World</h1>\n",
    "  <a class=\"link\" href=\"#\">This is link1</a>\n",
    "  <a class=\"link\" href=\"# link2\">This is link2</a>\n",
    " </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html_doc,'html.parser')\n",
    "#print(soup.prettify())\n",
    "header = soup.select('h1')\n",
    "print(header)\n",
    "print(header[0])\n",
    "print(header[0].text)\n",
    "\n",
    "print('=='*12)\n",
    "\n",
    "alink = soup.select('a')\n",
    "print(alink)\n",
    "for i in alink:\n",
    "    #print(i)\n",
    "    print(i.text)\n",
    "    print(i['href'])\n",
    "    \n",
    "print('=='*12)\n",
    "\n",
    "print(soup.select('#title')) # 寻找id 为title时，前面加#来修饰\n",
    "print(soup.select('.link')) #寻找class为link是，用 .link来\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
